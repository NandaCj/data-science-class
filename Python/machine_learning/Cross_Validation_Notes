Cross Validation :
	Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data 
	and evaluating 	them on the complementary subset of the data
	

Model Validation Techniques:

1.Holdout Method :
	Remove part of the Data from training set and use it for validation
	Generally 80 - 20 ratio is followed
	it still suffers from issues of high variance. 
	This is because it is not certain which data points will end up in the validation set 
		and the result might be entirely different for different sets.

2.K-Fold Cross Validation :
	By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias
	In K Fold cross validation, the data is divided into k subsets. 
	Now the holdout method is repeated k times, 
		such that each time, one of the k subsets is used as the test set/ validation set and 
		the other k-1 subsets are put together to form a training set
	The error estimation is averaged over all k trials to get total effectiveness of our model
	This significantly reduces bias as we are using most of the data for fitting, 
		and also significantly reduces variance as most of the data is also being used in validation set. 
	As a general rule and empirical evidence, K = 5 or 10 is generally preferred

3.Stratified K-Fold Cross Validation:
	 a slight variation in the K Fold cross validation technique is made, 
	 such that each fold contains approximately the same percentage of samples of each target class as the complete set, 
	 or in case of prediction problems, the mean response value is approximately equal in all the folds.
	 This variation is also known as Stratified K Fold

4.Leave-P-Out Cross Validation:
	This approach leaves p data points out of training data, 
	i.e. if there are n data points in the original sample then,
	n-p samples are used to train the model and p points are used as the validation set
	

The general procedure is as follows:

Shuffle the dataset randomly.
Split the dataset into k groups
For each unique group:
	Take the group as a hold out or test data set
	Take the remaining groups as a training data set
	Fit a model on the training set and evaluate it on the test set
	Retain the evaluation score and discard the model
Summarize the skill of the model using the sample of model evaluation scores


Variations on Cross-Validation
There are a number of variations on the k-fold cross validation procedure.

Three commonly used variations are as follows:

Train/Test Split: 
	Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.
LOOCV: 
	Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be 		the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.
Stratified: 
	The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a 		given categorical value, such as the class outcome value. This is called stratified cross-validation.
Repeated: 
	This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each 				repetition, which results in a different split of the sample.
	
	
	
	
	
	
	
	
	
	
	